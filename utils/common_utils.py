# -*- coding: utf-8 -*-
import torch, gc,random, copy, os
import numpy as np

IGNORE_INDEX=-100

SEP_TOKEN="<sep>"

DEFAULT_PAD_TOKEN = "[PAD]"
DEFAULT_EOS_TOKEN = "</s>"
DEFAULT_BOS_TOKEN = "<s>"
DEFAULT_UNK_TOKEN = "<unk>"

def print_once(message):
    if torch.distributed.is_initialized():
        if torch.distributed.get_rank() == 0:
            print(message, flush=True)
    else:
        print(message, flush=True)
        
def free_gpu():
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache() # release torch objects
    elif torch.npu.is_available():
        torch.npu.empty_cache()

def check_available_memory(device_index: int):
    """
    Returns available memory in GB for the given device index.
    Supports CUDA and NPU.
    """
    if torch.cuda.is_available():
        device = torch.device(f"cuda:{device_index}")
        props = torch.cuda.get_device_properties(device)
        used = torch.cuda.memory_allocated(device)
        free_gb = (props.total_memory - used) / 1024.0 / 1024.0 / 1024.0
        return free_gb
    elif torch.npu.is_available():
        import torch_npu
        device = f"npu:{device_index}"
        total_mem = torch_npu.npu.get_device_properties(device).total_memory
        used_mem = torch_npu.npu.memory_allocated(device)
        free_gb = (total_mem - used_mem) / 1024.0 / 1024.0 / 1024.0
        return free_gb
    else:
        # fallback to CPU (return some small default)
        return 0.0

def set_special_tokens(model, tokenizer, show_info=False):
    if tokenizer.pad_token is None and tokenizer.pad_token_id is None:
        print_once(f"WARNING: the pad token of the tokenizer is None")
        # We do not resize the vocab embedding, since it ruins the KL value with the ref_model
        tokenizer.pad_token_id = tokenizer.eos_token_id
        tokenizer.pad_token = tokenizer.eos_token
        # tokenizer.pad_token = tokenizer.decode(0)
        print_once(f">>> set pad token to {tokenizer.pad_token}")
        print_once(f">>> set pad token id to {tokenizer.pad_token_id}")

    model.config.pad_token_id = tokenizer.pad_token_id
    model.config.bos_token_id = tokenizer.bos_token_id
    model.config.eos_token_id = tokenizer.eos_token_id
    
    if show_info:
        print_once(tokenizer)

    return model, tokenizer

def compute_loglikelihood(logits, labels):
    """ compute the loglikelihood
    """
    batch_size, seq_length, vocab_size = logits.shape
    
    shift_logits = logits[..., :-1, :].contiguous()
    shift_labels = labels[..., 1:].contiguous()
    
    # Flatten the tokens
    loss_fct = torch.nn.CrossEntropyLoss(reduction='none')
    shift_logits = shift_logits.view(-1, vocab_size)
    shift_labels = shift_labels.view(-1)
    
    # Enable model parallelism
    shift_labels = shift_labels.to(shift_logits.device)
    loss = loss_fct(shift_logits, shift_labels).reshape(batch_size, -1) # [bs * seq_len]
    ignore_mask = labels != IGNORE_INDEX
    
    avg_loss = loss.sum(dim=-1) / ignore_mask.sum(dim=-1)

    return - avg_loss

def SFTwithKLTrainer(Trainer):
    """ with original alpaca instruct tuning data for KL norm override
        the loss function with additional KL norm by a reference
    """
    def compute_loss(self, model, inputs, return_outputs=False):
        # print_once(f"check inputs: {inputs}")  # debug infos
        model_outputs = model(
            input_ids=inputs['input_ids'],
            attention_mask=inputs['attention_mask']
        )
        logprob = compute_loglikelihood(model_outputs.logits, inputs['labels'])

def aggregate_rejection(seq: str)->str:
    # create a significant rejection to boost the self-play improvement.
    degradation = ["repeat", "drop", "rand_drop","none", "none", "none","none"]
    action = random.choice(degradation)
    out_seq = ""
    if action=="repeat":
        if len(seq)<10:   # too short, repeat the whole seuquence
            out_seq = f"{seq} {seq}"
        else: # repeat a random section
            start=random.randint(0,len(seq)//2 - 1)
            end=random.randint(start+1, len(seq))
            out_seq = f"{seq[:start]} {seq[start:end]} {seq[start:end]} {seq[end:]}"
    elif action == "drop" and len(seq)>10:  # drop a section
        if "," in seq:
            out_seq = seq[:seq.index(",")]
        elif "，" in seq:
            out_seq = seq[:seq.index("，")]
        else:
            out_seq = out_seq
    elif action == "rand_drop" and len(seq)>10:  # randomly drop some characters.
        number_to_drop = random.randint(1, len(seq)//4)
        indices_to_drop = random.sample(range(len(seq)), number_to_drop)
        tokens = [seq[i] for i in range(len(seq)) if i not in indices_to_drop]
        out_seq = "".join(tokens)
    else: # nothing changed
        out_seq = copy.deepcopy(seq)
    return out_seq

def get_path(args, path:str)->str:
    if path.startswith("/"):  # absolute path, no need to process
        return path
    else:  # relative path (based by nas_base_path)
        return os.path.join(args.nas_base_path, path)

def get_ranks(number_array):
    """
    return the array of rank
    """
    ascending_index = number_array.argsort()
    rank = np.zeros_like(ascending_index)
    for r in range(len(ascending_index)):
        rank[ascending_index[r]] = r
    return rank        

def truncate_encoded(inputs, max_length=500):
    """
    truncate the encoded inputs to max_length:
    inputs: dict of encoded inputs
        keys = ["input_ids","token_type_ids","attention_mask"]
    max_length: 500 for BERT default max_len=512
    """
    trunc_inputs = {"input_ids":inputs["input_ids"][:, :max_length],
                  "token_type_ids":inputs["token_type_ids"][:, :max_length],
                  "attention_mask":inputs["attention_mask"][:, :max_length]
                 }
    return trunc_inputs   

